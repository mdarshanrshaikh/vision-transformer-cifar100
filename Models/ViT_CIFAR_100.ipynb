{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La9GQBQO5y3O"
      },
      "outputs": [],
      "source": [
        "#Importing the required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLfQ7r8kJqjh"
      },
      "outputs": [],
      "source": [
        "import sys #to solve the IndexError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rKVd_xQWdde"
      },
      "source": [
        "1. ViT core components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrT5toKoohOl"
      },
      "source": [
        "A mechanism that calculates and applies a weighted sum of value vectors based on the scaled dot-product similarity between query and key vectors, performed simultaneously across multiple representation subspaces (heads)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g7R-Dw2WPte"
      },
      "outputs": [],
      "source": [
        "#Implementing the multi-head self-attention mechanism\n",
        "class MultiHeadAttention(nn.Module): #nn.Module is inherited for automatic parameter tracking\n",
        "  def __init__(self, embed_dim, num_heads):\n",
        "    super().__init__() #sets up module's internal state for parameter and submodule registration\n",
        "    self.num_heads = num_heads #stores the number of parallel attention heads\n",
        "    self.head_dim = embed_dim // num_heads #calculates the dimension of each individual head (num_heads * head_dim = embed_dim)\n",
        "    self.scale = self.head_dim ** -0.5 #calculates the scaling factor used in scaled dot-product attention: 1/sqrt(D_k).\n",
        "    #This prevents dot products from growing too large, which stabalizes the softmax function and thus the training process\n",
        "\n",
        "    #linear layers for Query, Key, Value & output projection\n",
        "    self.qkv = nn.Linear(embed_dim, embed_dim*3, bias=False)\n",
        "    self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, x): #forward function\n",
        "    #input shape: (batch_size, num_tokens, embed_dim)\n",
        "    B, N, C = x.shape #unpacks the input shape\n",
        "\n",
        "    #1. generate Q, K, V\n",
        "    #QKV shape: (B,N,3*C) -> split into 3 tensors\n",
        "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "    # Indices after reshape: 0=Batch(B), 1=Tokens(N), 2=QKV_Split(3), 3=Heads(H), 4=Head_Dim(D_h)\n",
        "\n",
        "    # qkv[0]: Q, qkv[1]: K, qkv[2]: V (all have shape: B, num_heads, num_tokens, head_dim)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "    #2. compute attention score: (Q @ K^T) * scale\n",
        "    # (B, H, N, D) @ (B, H, N, D) -> (B, H, N, D)\n",
        "    attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "    #3. softmax to get attention weights\n",
        "    attn = attn.softmax(dim=-1) #applies softmax function along the last dimension (N of the keys).\n",
        "    #this converts the logits into attention weights (probabilities) that sum to 1 for each query token.\n",
        "\n",
        "    #4. apply attention to V: (Attn @ V)\n",
        "    # (B, H, N, D) @ (B, H, N, D) -> (B, H, N, D)\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, C) #.transpose reverts the earlier permutation to bring the token (N) back to the second postion (B, N, H, D)\n",
        "\n",
        "    #5. output projection\n",
        "    x = self.proj(x)\n",
        "    return x #final output tensor of shape (B, N, C) which the result of the multi-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g50Z4eXDom97"
      },
      "source": [
        "A two-layer position-wise feed-forward network that first expands the feature dimension (e.g., $1 \\times \\text{to } 4 \\times$) then contracts it back to the original size, with a non-linearity (GELU) in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzfZXKyegs_f"
      },
      "outputs": [],
      "source": [
        "#feed forward network with 2 linear layers\n",
        "class MLP(nn.Module): #Multi-Layer Perceptron\n",
        "  def __init__(self, embed_dim, hidden_dim, dropout_rate=0.1): #hidden_dim means the expansion dimension\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(embed_dim, hidden_dim), #expansion\n",
        "        nn.GELU(), #Applies the Gausian Error Linear Unit non-linearity to the expanded features\n",
        "        #this introduces the complexity needed for the network to learn non-linear mappings\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(hidden_dim, embed_dim), #contraction\n",
        "        nn.Dropout(dropout_rate)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M1LQQfxouRu"
      },
      "source": [
        "The fundamental repeating unit of a Transformer encoder, consisting of a Multi-Head Attention sublayer and an MLP sublayer, each wrapped with a residual connection and Layer Normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWYyTYPUk_x0"
      },
      "outputs": [],
      "source": [
        "#a single transformer encoder block, consisting MSA & MLP\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #1. attention (with residual connection and layer normalization)\n",
        "    x = x + self.attn(self.norm1(x)) #the addition is a crucial step which helps with gradient flow and prevents information loss in deep networks.\n",
        "    #2. MLP (with residual connection and layer normalization)\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x #same shape as the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdMP2sCDo9XA"
      },
      "source": [
        "2. Patch Embedding and full ViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAd0KzuYo8_k"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size=32, patch_size=4, num_classes=100, embed_dim=192, depth=4, num_heads=3, mlp_ratio=4, dropout_rate=0.1):\n",
        "    #depth: number of transformer blocks\n",
        "    super().__init__()\n",
        "\n",
        "    num_patches = (img_size // patch_size) ** 2 #calculates the total number of patches created from the image\n",
        "    # E.g., a 32 x 32 image with 4 x 4 patches yields (32/4)^2 = 64 patches.\n",
        "    in_channels = 3 #RGB\n",
        "\n",
        "    #Calculate the MLP hidden layer\n",
        "    mlp_dim = int(embed_dim * mlp_ratio)\n",
        "\n",
        "    #1. patching embedding layer\n",
        "    #uses a conv2d with stride=patch_size, kernel_size=patch_size\n",
        "    self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    #2. class token (CLS)\n",
        "    #prepends a learnable parameter to the sequence\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    #3. positional encoding\n",
        "    #learns the spatial position of each batch + the CLS token\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim))\n",
        "\n",
        "    self.pos_dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    #4. transformer encoder\n",
        "    self.transformer_blocks = nn.Sequential(*[\n",
        "        TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate)\n",
        "        for _ in range(depth)\n",
        "    ])\n",
        "\n",
        "    #5. final normalization & head\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    # *** CRITICAL FIX/DEBUG: Hardcode 100 and print it to ensure the final layer is correct.\n",
        "    FINAL_CLASSES = 100\n",
        "    self.head = nn.Linear(embed_dim, FINAL_CLASSES)\n",
        "\n",
        "    print(f\"--- ViT INIT CHECK: Final Head Output Features: {self.head.out_features} (Expected 100) ---\", file=sys.stderr, flush=True)\n",
        "    # -----------------------------\n",
        "\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self): #helper method to initialize the weights of the layers and learnable parameters\n",
        "    #simple initialization scheme\n",
        "    nn.init.trunc_normal_(self.pos_embed, std=.02) #initializes the postional encoding with truncated normal distribution\n",
        "    nn.init.trunc_normal_(self.cls_token, std=.02) #initializes the CLS token with truncated normal distribution\n",
        "    self.apply(self._init_vit_weights) #recursively applies the custom initialization function _init_vit_weights to all submodules (Conv2D, Linear, LayerNorm, etc.)\n",
        "\n",
        "  def _init_vit_weights(self, m):\n",
        "    #linear layer's weights are initialized using a truncated normal distribution, and biases are set to 0.\n",
        "    if isinstance(m, nn.Linear):\n",
        "      nn.init.trunc_normal_(m.weight, std=.02)\n",
        "      if m.bias is not None:\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    #layer normalization layer's bias is set to 0 & weight is set to 1.\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "      nn.init.constant_(m.bias, 0)\n",
        "      nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.shape[0] #batch size\n",
        "\n",
        "    #1. Patch embedding\n",
        "    # (B, 3, 32, 32) -> (B, Embed_dim, 8, 8) (since 32/4=8)\n",
        "    x = self.patch_embed(x)\n",
        "    #flatten patches: (B, Embed_dim, H, W) -> (B, H*W, Embed_dim)\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "    #2. prepend CLS token (B, 1, Embed_dim)\n",
        "    cls_token = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "    #3. add positional encoding\n",
        "    x = x + self.pos_embed\n",
        "    x = self.pos_dropout(x)\n",
        "\n",
        "    #4. pass through transformer block\n",
        "    x = self.transformer_blocks(x)\n",
        "\n",
        "    #5. extract the CLS token\n",
        "    #take only the first token for classification\n",
        "    x = self.norm(x[:, 0])\n",
        "\n",
        "    #6. classification head\n",
        "    x = self.head(x)\n",
        "    return x #final logits for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0O1LaMy40pi"
      },
      "source": [
        "Truncated Normalization (or Truncated Normal Initialization) is a method of initializing the weights and biases of a neural network layer by drawing values from a Gaussian (Normal) distribution but strictly restricting the values to lie within a defined range. The key idea is to prevent the occurrence of extreme outliers that can destabilize the early stages of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkygmXM1_-Rr"
      },
      "source": [
        "Training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHoBh5br4iJc"
      },
      "outputs": [],
      "source": [
        "#setting the configurations\n",
        "IMG_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 100\n",
        "EMBED_DIM = 192\n",
        "DEPTH = 4\n",
        "NUM_HEADS = 3\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 1\n",
        "#these configurations are only used because of hardware limitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqYb3F5SAhTs"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUeiI6I2AkED"
      },
      "outputs": [],
      "source": [
        "#data preprocessing and augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4), #zero-pads the border by 4 pixels and then randomly crops a 32 X 32 region\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(), #converts the image into tensor and also scales the value into the range [0.0, 1.0]\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #standardizes the tensor by applying channel-wise normalization using the provided mean & standard deviation.\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Hhm6S-CY1G"
      },
      "outputs": [],
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SEtx8qaClIJ"
      },
      "outputs": [],
      "source": [
        "#loading CIFAR-100 dataset\n",
        "train_datasets = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_datasets = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhYwBrZDDTgo"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_datasets, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoFyPeLdDmyP",
        "outputId": "63f69397-3730-4555-e719-11d29dd68c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--- ViT INIT CHECK: Final Head Output Features: 100 (Expected 100) ---\n"
          ]
        }
      ],
      "source": [
        "#initializing model, loss & optimizer\n",
        "model = VisionTransformer(\n",
        "    img_size=IMG_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    depth=DEPTH,\n",
        "    num_heads=NUM_HEADS\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrrOGUJ9D9Tm",
        "outputId": "4eecc541-8f39-45cd-c5fd-93a61a848798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 1.82M\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikGiPycWElBU"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpMVyNoUE8N5"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n",
        "#AdamW is an enhanced version of the traditional Adam (Adaptive Moment Estimation) optimizer, specifically designed to properly handle weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvksNcA0Iymo"
      },
      "outputs": [],
      "source": [
        "checked_output_shape = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mBq3sWkFTCx",
        "outputId": "2b2945bb-a1b0-4dea-d7f6-c73f266c0e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 | Train Loss: 3.5911 | Test Accuracy: 16.41%\n"
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(train_loader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # --- DEBUGGING CHECK ---\n",
        "    if not checked_output_shape:\n",
        "      print(f\"--- DEBUG SHAPE CHECK ---\")\n",
        "      print(f\"Model Output Shape (logits): {outputs.shape}\")\n",
        "      print(f\"Expected Class Dimension: {NUM_CLASSES}\")\n",
        "      print(f\"Labels Shape: {labels.shape} | Max Label Index: {labels.max().item()}\")\n",
        "      print(f\"-------------------------\")\n",
        "      checked_output_shape = True\n",
        "    # --- END DEBUGGING CHECK ---\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "  #evaluation\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (labels==predicted).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct/total\n",
        "  print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}