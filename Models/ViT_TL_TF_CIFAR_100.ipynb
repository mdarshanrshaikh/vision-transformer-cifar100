{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YmXmrl8joygb"
      },
      "outputs": [],
      "source": [
        "#Importing required libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#configurations\n",
        "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\" #used mobilenetv3 model instead of ViT because of compatibility issue\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_EPOCHS = 1"
      ],
      "metadata": {
        "id": "bx8ODp42qn4B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNetV3-Large (100% width, 224×224 input) — a lightweight convolutional neural network trained on ImageNet that outputs a feature vector (no classification head) for transfer learning."
      ],
      "metadata": {
        "id": "ZG1ejS19WNDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading & preprocessing"
      ],
      "metadata": {
        "id": "Z5mU9eS0rkP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(image, label): #resizes & normalizes image data for the ViT model\n",
        "  #converting image and label to float\n",
        "  image = tf.cast(image, tf.float32)\n",
        "\n",
        "  #resizing the 32X32 CIFAR-100 images to 224x224 size\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "  #Normalizing the image\n",
        "  image = image/255.0\n",
        "\n",
        "  #one-hot encode the label\n",
        "  label = tf.one_hot(label, NUM_CLASSES)\n",
        "\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "6qLPiCpbrgvu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar100(): #loads & prepares CIFAR-100 dataset\n",
        "  #load data\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "\n",
        "  #converting labels to 1D array\n",
        "  y_train = np.squeeze(y_train)\n",
        "  y_test = np.squeeze(y_test)\n",
        "\n",
        "  #create tensorflow datasets\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "  #applying preprocessing, shuffling & batching\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "  train_ds = train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE) #tells TensorFlow to use as many CPU threads as it thinks optimal for running this mapping in parallel.\n",
        "  train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(AUTOTUNE) #lets TensorFlow prepare the next batch while the GPU is still training on the current one.\n",
        "\n",
        "  test_ds = test_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
        "  test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "  return train_ds, test_ds"
      ],
      "metadata": {
        "id": "U9if-CBUsicZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.data.AUTOTUNE is a TensorFlow constant that lets the tf.data pipeline automatically decide how many parallel threads to use for operations like map() and how many batches to prefetch."
      ],
      "metadata": {
        "id": "sdsQIw_RWapD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition & Transfer learning"
      ],
      "metadata": {
        "id": "qd_JhJ77uUpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transfer_model(): #builds a keras sequential model using the ViT feature extractor from TF hub\n",
        "  #load the pre-trained ViT feature extractor\n",
        "  feature_extractor_layer = hub.KerasLayer(\n",
        "      MODEL_URL,\n",
        "      input_shape = (IMG_SIZE, IMG_SIZE, 3),\n",
        "      trainable = False, #freezing the feature extraction layer\n",
        "      name = 'vit_feature_extractor'\n",
        "  )\n",
        "\n",
        "  #input layer\n",
        "  inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='input_image')\n",
        "\n",
        "  #wrapping the feature extractor in lambda to ensure TF graph compatibility/had to add it because of the error\n",
        "  x = tf.keras.layers.Lambda(lambda img: feature_extractor_layer(img))(inputs)\n",
        "\n",
        "  #classification head\n",
        "  x = tf.keras.layers.Dropout(0.3)(x)\n",
        "  outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='classification_head')(x)\n",
        "\n",
        "  #model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cifar100_tl_tf_vit')\n",
        "\n",
        "  #compile the model\n",
        "  model.compile(\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  model.summary(print_fn=lambda x: print(x, file=sys.stderr, flush=True))\n",
        "  return model"
      ],
      "metadata": {
        "id": "l6HcHuhPuRtG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "d2hHH_gJxqUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = load_cifar100()\n",
        "model = build_transfer_model()\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs = NUM_EPOCHS,\n",
        "    validation_data = test_ds,\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "M8zQMLrNxlDf",
        "outputId": "32a883a3-5193-437e-b581-875f05256170"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model: \"cifar100_tl_tf_vit\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ input_image (InputLayer)        │ (None, 224, 224, 3)    │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ lambda_2 (Lambda)               │ (None, 1280)           │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout_1 (Dropout)             │ (None, 1280)           │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ classification_head (Dense)     │ (None, 100)            │       128,100 │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 128,100 (500.39 KB)\n",
            " Trainable params: 128,100 (500.39 KB)\n",
            " Non-trainable params: 0 (0.00 B)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1800s\u001b[0m 2s/step - accuracy: 0.5225 - loss: 2.5734 - val_accuracy: 0.6398 - val_loss: 2.3893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "-xlSqalYz1g8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe723381-d5ab-49b6-87c8-e9f28d5d272d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 2s/step - accuracy: 0.6359 - loss: 2.4254\n",
            "Test Loss: 2.3893, Test Accuracy: 63.98%\n"
          ]
        }
      ]
    }
  ]
}